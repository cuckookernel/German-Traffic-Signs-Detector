{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report on Model3 \n",
    "\n",
    "This model implements the original LeNet architecture from the classic paper by Yann LeCun \n",
    "\n",
    "http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf.\n",
    " \n",
    "The code is contained within `/tf_lenet.py`\n",
    "\n",
    "We tried to stay as close to the original as posible with two exceptions: \n",
    "\n",
    " 1.  As the images are RGB there is the design choice of whether they should be:\n",
    "    1. converted to monochrome, essentially applying  `tf.reduce_mean` along the color-channels dimension \n",
    "    2. treated as RGB by setting the shape of the filters in the first convolutional layer to  [5,5,3,16] instead of the original [5, 5, 1, 16] shape specified in the original paper.\n",
    "     We actually tried both posibilities and can easily switch between the two by means of the `drop_colors` hyper-parameter,\n",
    "     If set to 1 then images are converted to monochrome in the first layer, if set to 0 then the [5,5,3,16] filters are used.\n",
    "     Se first few lines of `tf_lenet.layer_c1` for the whole story.\n",
    "     \n",
    "     \n",
    " 2. Due to the small number of images we had for training, it was necessary to do employ some regularization technique. We chose to do drop out on the first fully connected layer. The `dropout_rate` hyper-parameter controls what portion of conections are dropped. See `tf_lenet.fully_connected` for details.\n",
    " \n",
    " ## A Note about non-linearity function and the final layer\n",
    " \n",
    "Most recent implementations of the LeNet architecture out there are not really faithful to the original in at least two respects: \n",
    "\n",
    "  1. They use `relu` units after two introduce non-linearities after most layers instead of the $A \\cdot tanh$ function that LeCun used in his design. \n",
    "\n",
    "  2. They use a standard 'sigmoidal' fully connected layer for the final layer, instead of LeCun's proposed gaussian connections.  \n",
    "  \n",
    "  \n",
    "We have followed the original in both respects but also tried the more modern versions and have noted with some surprise that the original version works better, or at least is to find good hyper parameters for easier. We suspect this might be due to the limited number of training images used. \n",
    "     \n",
    "     \n",
    "      \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
